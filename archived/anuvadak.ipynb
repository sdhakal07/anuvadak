{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da71be71-db85-42e9-9210-a245cd2c1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499a39c-7360-4d8b-8626-0da89782dcd3",
   "metadata": {},
   "source": [
    "## Parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5520eb5-c673-4bc0-8e81-913661bee5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Who?', '[Start]को? [end]'), ('Hide.', '[Start]लुकाउनुहोस्। [end]'), ('Hide.', '[Start]लुक। [end]'), ('Stay.', '[Start]बस्नुहोस्। [end]'), ('Hello!', '[Start]नमस्ते! [end]')]\n"
     ]
    }
   ],
   "source": [
    "text_file = r\"npi.txt\"\n",
    "with open(text_file, encoding=\"utf8\") as f:\n",
    "    lines=f.read().split(\"\\n\")[:-2]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng,nep= line.split(\"\\t\")[:2]\n",
    "    nep = \"[Start]\"+ nep+\" [end]\"\n",
    "    text_pairs.append((eng,nep))\n",
    "print(text_pairs[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adee76f-4ec3-48c1-94bb-0a24bd85544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('it happened on the third day, that behold, a man came out of the camp from Saul, with his clothes torn, and earth on his head: and so it was, when he came to David, that he fell to the earth, and showed respect.', '[Start] तब तेस्रो दिनमा एउटा जवान सैनिक सिकलगमा आयो। त्यो मानिस शाऊलको छाउनीबाट आएको थियो। त्यसका लुगाहरू च्यतिएको र शिरमा मैला लागेको थियो। त्यसले दाऊदको अघि धोप्टो परेर उनलाई सम्मान गर्न दण्डवत् गर्यो। [end]'), ('David said to him, \"Where do you come from?\" He said to him, \"I have escaped out of the camp of Israel.\"', '[Start] दाऊदले त्यसलाई सोधे, “तिमी कहाँबाट आयौ?” त्यस मानिसले जवाफ दियो, “म इस्राएली पालबाट आउँदैछु।” [end]'), ('David said to him, \"How did it go? Please tell me.\" He answered, \"The people have fled from the battle, and many of the people also have fallen and are dead; and Saul and Jonathan his son are dead also.\"', '[Start] दाऊदले भने, “मलाई भन, के भयो?” त्यसले भन्यो, “हाम्रा सबै सैनिकहरू भागे। धेरै मानिसहरू मारिए। शाऊल र तिनका छोरा जोनाथन पनि मरे।” [end]'), ('David said to the young man who told him, \"How do you know that Saul and Jonathan his son are dead?\"', '[Start] दाऊदले त्यस सैनिकलाई भने, “तिमीले कसरी जान्यौ शाऊल र जोनाथन मरेको कुरा?” [end]'), ('The young man who told him said, \"As I happened by chance on Mount Gilboa, behold, Saul was leaning on his spear; and behold, the chariots and the horsemen followed hard after him.', '[Start] त्यो सैनिकले उत्तर दियो, “म गिल्बो पर्वतमाथि थिएँ। मैले शाऊललाई आफ्नो भालामा झुकेको देखें। पलिश्ती सैनिकहरू घोडा र रथहरूमा चढेर शाऊलको नजिक आउँदै थिए। [end]')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "excel_file= r\"C:\\Users\\Leapfrog\\Desktop\\Nlp\\Transformer\\english-nepali.xlsx\"\n",
    "sheet_name = \"Sheet1\"\n",
    "df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "\n",
    "rows_as_tuples = [(row[0],f\"[Start] {row[1]} [end]\") for row in df.values]\n",
    "\n",
    "print(rows_as_tuples[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ddddb7b-bb46-42ce-aeb1-914cfef2507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs =text_pairs+rows_as_tuples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325b489-5a93-4d89-891a-c0cc4dfcc755",
   "metadata": {},
   "source": [
    "# Here's what our sentence pairs look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc24362d-bf9d-485c-8de3-eeb1818a2f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The waters surrounded me, even to the soul. The deep was around me. The weeds were wrapped around my head.', '[Start] “पानीले मलाई मेरो प्राण सम्मै घेर्यो, गहिरो सगर मेरो चारैतिर थियो, मेरो टाउको भरि समुद्रका झ्याउहरु बेहेरिएका थिए। [end]')\n",
      "('Now when they had departed, behold, an angel of the Lord appeared to Joseph in a dream, saying, \"Arise and take the young child and his mother, and flee into Egypt, and stay there until I tell you, for Herod will seek the young child to destroy him.\"', '[Start] ज्योतिषीहरू गएपछि, परमप्रभुको एउटा दूत सपनामा यूसुफकहाँ देखा परे। दूतले भने, “उठ, साना बालक र उहाँकी आमालाई साथमा लिएर मिश्रदेशमा जाऊ। हेरोदले साना बालकहरूलाई खोज्न लागिरहेकोछ। उसले सानो बालकलाई मार्न चाहन्छ। मैले नभनेसम्म तिमी त्यहीं बस्नु।” [end]')\n",
      "('This is the inheritance of the children of Zebulun according to their families, these cities with their villages.', '[Start] यसरी, यी शहरहरू अनि ती वरिपरिका खेतहरू जबूलूनलाई दिए। जबूलूनका प्रत्येक कुलले भूमिका आफ्ना अंश प्राप्त गरे। [end]')\n",
      "('\"please don\\'t be Lebanese\" ', '[Start] कृपया प्यालेष्टाइनी नहोऊ  [end]')\n",
      "('It also tweaked the kingâ€™s image so that it appeared he was smiling while holding a glass of beer. ', '[Start] साथै बियरको गिलास हातमा लिएर मुस्कुराइरहेको जस्तो देखिने बनाएर राजाको तस्वीरलाई तोडमोड गर्यो। [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a4614-a68c-4162-b71d-fae0a386a613",
   "metadata": {},
   "source": [
    "## Let's split the sentence pairs into a training set , a validation set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ba2807e-2e50-4dad-809c-b36840bca6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34707 total pairs\n",
      "24295 training pairs\n",
      "5206 validation pairs\n",
      "5206 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15*len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2*num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples: num_train_samples+num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples +num_val_samples:]\n",
    "\n",
    "print(f'{len(text_pairs)} total pairs')\n",
    "print(f'{len(train_pairs)} training pairs')\n",
    "print(f'{len(val_pairs)} validation pairs')\n",
    "print(f'{len(test_pairs)} test pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5ff13-c3fd-4b28-99d8-2ccf5935a10a",
   "metadata": {},
   "source": [
    "# Vectorizing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6764660a-dda6-4c20-a947-b5586c388d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~?\n"
     ]
    }
   ],
   "source": [
    "strip_chars = string.punctuation + \"?\"\n",
    "print(strip_chars)\n",
    "strip_chars = strip_chars.replace(\"[\",\"\")\n",
    "strip_chars = strip_chars.replace(\"]\",\"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 10\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.string.lower(input_string)\n",
    "    return tf.string.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens = vocab_size, output_mode = \"int\", output_sequence_length=sequence_length,)\n",
    "nep_vectorization = TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = sequence_length+1,\n",
    "    #standardize = custom_standardization,\n",
    ")\n",
    "\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_nep_texts = [pair[1] for pair in train_pairs]\n",
    "# print(train_nep_texts)\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "nep_vectorization.adapt(train_nep_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e01f5c2-4319-40d6-85e8-5c9375593100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng,nep):\n",
    "    eng = eng_vectorization(eng)\n",
    "    nep = nep_vectorization(nep)\n",
    "    return({\"encoder_inputs\": eng, \"decoder_inputs\": nep[:, :-1],}, nep[:,1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, nep_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    nep_texts = list(nep_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, nep_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd10a75-75ce-4c5c-bd0d-247ffb0f5b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (2, 10)\n",
      "inputs[\"decoder_inputs\"].shape: (2, 10)\n",
      "targets.shape: (2, 10)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f'targets.shape: {targets.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074e86a-f9f4-4116-b595-e7f0da8f32c0",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49cd2a44-729f-45fc-b1e0-865b93610603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_din, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads = num_heads, key_dim=embed_din\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation = \"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "        \n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value = inputs, key= inputs, attention_mask= padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs+attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad054a8-7d5c-4351-9f25-f3f220d0f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        \n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
    "        )\n",
    "        \n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        # Generate causal mask\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        \n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"float32\")\n",
    "            combined_mask = causal_mask * padding_mask  # Logical AND for masks\n",
    "        else:\n",
    "            combined_mask = causal_mask\n",
    "\n",
    "        # Self-attention block\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        # Encoder-decoder attention block\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=combined_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        # Dense projection\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        sequence_length = input_shape[1]\n",
    "\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"float32\")  # Causal mask (lower triangular)\n",
    "        mask = tf.reshape(mask, (1, sequence_length, sequence_length))\n",
    "        mask = tf.tile(mask, [batch_size, 1, 1])  # Tile across the batch size\n",
    "        return mask        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c927303-f45c-4372-90f5-a82045714c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim = vocab_size, output_dim = embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim = sequence_length, output_dim= embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit = length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedding_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedding_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask = None):\n",
    "        return tf.math.not_equal(inputs,0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba6c28-3bc1-467a-8a59-f4a9403b7c79",
   "metadata": {},
   "source": [
    "## Training a Model for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54177c4b-26f9-483a-8d12-51e242650ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 512\n",
    "num_heads = 4\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name =\"encoder_inputs\")\n",
    "x= PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,),dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name =\"decoder_state_inputs\")\n",
    "\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x,encoded_seq_inputs)\n",
    "x= layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd487b-9d81-4136-8e56-7a17edee295e",
   "metadata": {},
   "source": [
    "## Training Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c69765ad-7911-42ab-9c7e-946c07cc0d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   3842560     ['encoder_inputs[0][0]']         \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   1315840     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 15000)  10065816    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,224,216\n",
      "Trainable params: 15,224,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e37fcea9-5e94-4a80-953c-211169037b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/transformer/transformer_encoder/multi_head_attention/softmax/add/BroadcastGradientArgs' defined at (most recent call last):\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Leapfrog\\AppData\\Local\\Temp\\ipykernel_23300\\3062792833.py\", line 1, in <module>\n      transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/transformer/transformer_encoder/multi_head_attention/softmax/add/BroadcastGradientArgs'\nIncompatible shapes: [2,4,10,10] vs. [2,2,10,10]\n\t [[{{node gradient_tape/transformer/transformer_encoder/multi_head_attention/softmax/add/BroadcastGradientArgs}}]] [Op:__inference_train_function_9549]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\oenv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/transformer/transformer_encoder/multi_head_attention/softmax/add/BroadcastGradientArgs' defined at (most recent call last):\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Leapfrog\\AppData\\Local\\Temp\\ipykernel_23300\\3062792833.py\", line 1, in <module>\n      transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"C:\\Users\\Leapfrog\\anaconda3\\envs\\oenv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/transformer/transformer_encoder/multi_head_attention/softmax/add/BroadcastGradientArgs'\nIncompatible shapes: [2,4,10,10] vs. [2,2,10,10]\n\t [[{{node gradient_tape/transformer/transformer_encoder/multi_head_attention/softmax/add/BroadcastGradientArgs}}]] [Op:__inference_train_function_9549]"
     ]
    }
   ],
   "source": [
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oenv",
   "language": "python",
   "name": "oenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
